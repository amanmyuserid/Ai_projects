# -*- coding: utf-8 -*-
"""smote_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GHrCeR6B-84X2xGFUIScV7Ohu-AIEo3h
"""

# Commented out IPython magic to ensure Python compatibility.
from collections import Counter

import numpy as np
import random

from sklearn.neighbors import NearestNeighbors
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.datasets import make_blobs

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics
import math

import matplotlib.pyplot as plt
# %matplotlib inline
'''
def plot_func(samples, labels, centroids=None, fuzzy_samples=None, fuzzy_values=None, file_name=None, title=None):
	plt.figure(figsize=(4, 4))
	plt.subplots_adjust(bottom=.05, top=.9, left=.05, right=.95)
	plt.subplot(111)
	plt.title(title)
	plt.scatter(samples[:, 0], samples[:, 1], marker='o', c=labels,
	            edgecolor='k')
	if centroids is not None:
		plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', c='#ff0000',
	            s = 50, edgecolor='k')
	
	if fuzzy_samples is not None:
		plt.scatter(fuzzy_samples[:, 0], fuzzy_samples[:, 1], marker='^', c='#00ff00',
	            s = 50, edgecolor='k')
	
	if fuzzy_values is not None:
		for fuzzy_sample,fuzzy_value in zip(fuzzy_samples,fuzzy_values):
			plt.annotate("%.2f" % round(fuzzy_value,2), # this is the text
				(fuzzy_sample[0], fuzzy_sample[1]), # this is the point to label
				textcoords="offset points", # how to position the text
				xytext=(0,10), # distance from text to points (x,y)
				ha='center') # horizontal alignment can be left, right or center
	if not file_name == None:
		plt.savefig(file_name)
	else:
		plt.show()
'''
n_features = 2
n_samples_1 = 2000
n_samples_2 = 200
# n_samples_2 = 200
centers = [[0.0, 0.0], [4.5, 4.5]]
clusters_std = [1, 3]
X_orig, y_orig = make_blobs(n_samples=[n_samples_1, n_samples_2],
                  centers=centers,
                  n_features=n_features,
                  cluster_std=clusters_std,
                  random_state=2, shuffle=False)

X_train, X_test, y_train, y_test = train_test_split(X_orig, y_orig, test_size=0.33, random_state=42, stratify=y_orig)

for i in range(len(y_train)):
    if y_train[i] == 0:
        y_train[i] = -1
for j in range(len(y_test)):
    if y_test[j] == 0:
        y_test[j] = -1    


'''
plot_func(X_train,y_train, title="Original")

X_train.shape

y_train.shape
'''
"""## Smote"""

! pip install imblearn

from imblearn.over_sampling import SMOTE, BorderlineSMOTE

X_smote, y_smote = SMOTE(random_state=42).fit_resample(X_train,y_train)

#plot_func(X_smote, y_smote, title='SMOTE')

#y_smote.shape

X_borderline, y_borderline = BorderlineSMOTE(random_state=42).fit_resample(X_train,y_train)

#plot_func(X_borderline, y_borderline, title='Borderline SMOTE')

#y_borderline.shape




oversampling_methods = ["Smote", "Borderline"]

for oversampling_method in oversampling_methods:

    train_data = np.concatenate((X_train, y_train.reshape(-1,1)), axis=1).tolist()
    
    if oversampling_method == 'Smote':

         X_oversampled = X_smote
         y_oversampled = y_smote
    else:
         X_oversampled = X_borderline
         y_oversampled = y_borderline


    oversampled_data = np.concatenate((X_oversampled, y_oversampled.reshape(-1,1)), axis=1).tolist()

    orig_majority_data = []
    orig_minority_data = []
    for iter, arr in enumerate(train_data):
        if arr[2] == 0:
            orig_majority_data.append(train_data[iter])
        else:
            orig_minority_data.append(train_data[iter])    


    
    orig_plus_synthetic_minority_data = []
    
    for iter, arr in enumerate(oversampled_data):
        if arr[2] == 1:
            orig_plus_synthetic_minority_data.append(oversampled_data[iter])

    
    
    for i in range(len(orig_minority_data)):
        for j in range(len(orig_plus_synthetic_minority_data)):
            if np.sum(np.array(orig_minority_data[i]) - np.array(orig_plus_synthetic_minority_data[j])) == 0:
                del orig_plus_synthetic_minority_data[j]
            break      

    synthetic_minority_data  = orig_plus_synthetic_minority_data

    
    print("\n")
    print("=="*32, oversampling_method, '=='*32)
    
    fig, a = plt.subplots(1,3,figsize=(20,4))
    x = np.arange(1,5)

    a[0].scatter(X_train[:,0], X_train[:,1], marker='o', c=y_train,
                  edgecolor='k')
    a[0].set_title('Orignial Dataset')
    
    for i in range(len(synthetic_minority_data)):
        synthetic_minority_data[i][2] = 0
    total_data = orig_majority_data + synthetic_minority_data + orig_minority_data 
    total_data = np.array(total_data)

    a[1].scatter(total_data[:,0], total_data[:,1], marker='o', c=total_data[:,2].astype(int),
                  edgecolor='k')
    a[1].set_title('Generating Sample')

    a[2].scatter(X_oversampled[:,0], X_oversampled[:,1], marker='o', c=y_oversampled,
                  edgecolor='k')
    a[2].set_title('Resampled Dataset')

    print('\n')

    plt.show()

